---
layout: post
title: Deriving Soft Actor Critic
subtitle: Going from zero to one of the key RL algorithms - Soft Actor Critic (SAC)
image: TODO
bibliography: papers.bib
tags: [robotics, research, mathsy, long]
header-includes:
  - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
  - \usepackage{amssymb}
  - \usepackage{amsmath}
  - \usepackage{mathtools}
  - \usepackage{amsthm}
  - \DeclareMathOperator{\EX}{\mathbb{E}}% expected value
---

Soft Actor Critic (SAC) is a model-free Reinforcement Learning (RL)
method developed by Tuomas Haarnoja in a series of papers culminating
in [Soft Actor-Critic Algorithms and Applications](https://arxiv.org/pdf/1812.05905).
It is in my opinion one of the seminal RL papers for continuous control
and has inspired a gread deal of my work.

In this blog post I'll go over the theory and derivation behind SAC,
from zero to a fully-derived algorithm without skipping (almost) any 
of the mathematical details. This time I haven't included an implmentation
of my own, but a good one to go along with this blog post is [denisyarats/pytorch_sac](https://github.com/denisyarats/pytorch_sac).

*Note: If you are familiar with RL basics, feel free to skip ahead*

**Table of contents**
* TOC
{:toc}

Problem
=================
We want a robot with limited perceptive abilities to learn to achieve
a given task. Let's formulate this mathematically and ground it in 
the real world via a beer opening robot:

* The world has a state at a given time $s_t \in S$. This state space
can consist of anything meaningful to the robot: the position of the
beer bottle? how full is it? how thursty is the human we need to serve?
* Unfortunately, robots can't open beer bottles by just thinking about
it really hard, instead they have to act in the world via actions $a_t \in A$.
For our robot, actions could be a 4-dimensional torque vector to control all
joints of the robot arm.
* To incentivise the robot to achieve its goal, we give it a reward for each
timestep $r_t \in \mathbb{R}$. This could be a function of the state and action
$r_t := r(s_t, a_t)$ or could be something more abstract like our human
telling us how happy he/she is. The key assumption is that the robot only
gets a scalar reward $r_t$ at each timestep without knowing how it is calculated.

![Interaction diagram of our beer opening task. Image generated with Midjourney](/img/blog/2023-06-27-sac/beer_opener_robot.png)

The goal of our agent now is to maximize the cumulative reward by chosing the optimal sequence of actions:

<a id="base_objective"></a>
$$ \max_{a_0, a_1, ..., a_T} \sum_{t=0}^T r_t $$

> Note: To be precise this is called a finite-horizon problem as we are only considering rewards up to a fixed timestep T. Naturally this has its limitations and often we want to consider infinite horizon problems. Although most of the content of this blog applies to such problems, the theory is more difficult to develop.

Technically our reward can depend on a number of factors from previous
timesteps (e.g. did the human think I look cool while opening his/her beer). However, that quickly becomes an exponentially harder problem, especially from a theoretical sense. To make the problem more tractable and our lives easier, we will make an assumption:

<a id="markov_assumption"></a>
> **Assumption 1**: Given a sequence of state-action pairs $(s_t, a_t) \ \ \ \forall t \in [0, T]$, the transition distribution is fully defined by $p(s_{t+1} | s_t, a_t)$. This is known as the [Markov Assumption](https://en.wikipedia.org/wiki/Markov_property).

In English: the last state $s_t$ contains all the useful information for our problem. From this we automatically get

<a id="reward_corollary"></a>
> **Corollary 1**: The reward is defined as $r: S \times A \rightarrow \mathbb{R}$


Visually this can be look as the following Markov Chain:

![The Markov Chain of our finite-horizon RL problem](/img/blog/2023-06-27-sac/rl_markov_chain.drawio.png)

With this assumption we can now redefine our [objective function](#base_objective) as

<a id="base_objective_2"></a>
$$ \max_{a_0, a_1, ..., a_T} \sum_{t=0}^T r(s_t, a_t) $$

Still, this is incorrectly defined. Why? Because we defined our transition dynamics as a probabilistic distribution in [Assumption 1](#markov_assumption). To tackle that we have to define our objective above as an expectation over the rewards:

<a id="base_objective_3"></a>
$$ \max_{a_0, a_1, ..., a_T} \sum_{t=0}^T \mathbb{E}_{s_{t+1} \sim p(s_{t} | s_{t-1}, a_{t-1})} r(s_t, a_t) $$

Since we're already dealing with probabilistic distributions and ultimately want our results to fit in modern machine learning, we can constrain our actions to be coming from a set of possible policies:

<a id="policy_assumption"></a>
> Assumption 2: Actions are sampled from a probabilistic feedback policy $\pi$ parametarised by $\theta \in \mathbb{R}^d$ and conditioned only on the current state $s_t:
> $$a_t \sim \pi(a_t | s_t; \theta)

Thus our final (and I mean it this time) objective becomes

<a id="base_objective_final"></a>
$$ \max_{a_0, a_1, ..., a_T} \sum_{t=0}^T \mathbb{E}_\substack{s_{t+1} \sim p(s_{t} | s_{t-1}, a_{t-1}) \\ a_t \sim \pi(a_t | s_t; \theta)} r(s_t, a_t) $$

Bellman Equations
==========================

These problems get really messy really fast. Luckily we have a simple unified way to think about these problmes - via the lens of value functions:

<a id="value_func_definition"></a>
> Definition 1: The value of a state $s_t$ following policy $\pi$ is given by
> $$ V^\pi(s_t) = \sum_{n=t}^T \mathbb{E}_\substack{s_{n+1} \sim p(s_{n} | s_{n-1}, a_{n-1}) \\ a_n \sim \pi(a_n | s_n; \theta)} r(s_n, a_n) $$

